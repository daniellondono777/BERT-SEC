##############################################################################################################################   
#
#   Class for uploading .txt files from the SEC to the cloud
#   Author: D. Santiago Londono
#
#   Project 4359 - BERT SEC
#
##############################################################################################################################

import sys
import re
import requests
from bs4 import BeautifulSoup
import time


class TextUploader():

    #
    #   Constructor method of the class
    #   @params
    #       urls: list - List of urls from the SEC to fetch and operate
    #
    def __init__(self, path: str) -> None:
        self.path = path
        self.source_url = 'https://www.sec.gov/Archives/'
        self.headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }
        self.urls = []
        with open(path, 'r') as file:
            for line in file:
                if line != '':
                    self.urls.append(line)
    

    #
    #   For a given string from the fetched SEC list, retrieves the URL of the .idx file
    #   @params
    #       string: str - A line from the results.txt file generated by the daemon
    #
    def scrutinize_(self, string)-> list:
        pattern = r'edgar(.*?)txt'

        match = re.search(pattern, string, re.DOTALL)

        if match:
            return match.group(0)  
        else:
            return None
    

    #
    #   (A) Returns the list containing the absolute URL to the filing
    #   @params
    #       string: str - For a given string from the fetched SEC list, retrieves the URL of the .idx file
    #
    def get_full_url_(self):
        return [ self.source_url + self.scrutinize_(i) for i in self.urls]
    

    #
    #   Obtains text files from (A) and stores them in the folder
    #   @params
    #
    def get_filings_(self):
        for url in self.get_full_url_():
            r = requests.get(url, headers=self.headers)
            try:
                soup = BeautifulSoup(r.text, 'html.parser')
                paragraphs = soup.find_all('p')
                with open('filing.txt', 'w', encoding='utf-8') as file:
                    for paragraph in paragraphs:
                        file.write(paragraph.get_text() + '\n')  # Write the paragraph content with line breaks
                break
            except:
                with open('filing.txt', 'w', encoding='utf-8') as file:
                    file.write(r.text)
                break
            time.sleep(10)


uploader = TextUploader(sys.argv[1])
uploader.get_filings_()